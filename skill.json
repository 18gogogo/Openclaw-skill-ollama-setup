{
  "name": "qwen-nvidia-model-config",
  "version": "1.1.0",
  "description": {
    "zh_TW": "OpenClaw Qwen2.5 與 NVIDIA MiniMax M2.1 模型配置模板，包含常見模型錯誤解決方案及 Modelfile 設定指南",
    "en": "OpenClaw Qwen2.5 and NVIDIA MiniMax M2.1 model configuration template with common error solutions and Modelfile setup guide"
  },
  "author": "OpenClaw Community",
  "license": "MIT",
  "keywords": ["qwen", "qwen2.5", "nvidia", "model", "config", "ollama", "openclaw", "modelfile"],
  "homepage": "https://github.com/openclaw/skill-qwen-nvidia-model-config",
  "repository": {
    "type": "git",
    "url": "https://github.com/openclaw/skill-qwen-nvidia-model-config"
  },
  "config": {
    "merge": {
      "gateway": {
        "auth": {
          "mode": "token",
          "token": "${GATEWAY_TOKEN}"
        },
        "port": 18789,
        "bind": "lan"
      },
      "channels": {
        "telegram": {
          "enabled": true,
          "botToken": "${TELEGRAM_BOT_TOKEN}"
        }
      },
      "env": {
        "NVIDIA_API_KEY": "${NVIDIA_API_KEY}"
      },
      "models": {
        "mode": "merge",
        "providers": {
          "nvidia-minimax": {
            "baseUrl": "https://integrate.api.nvidia.com/v1",
            "apiKey": "${NVIDIA_API_KEY}",
            "api": "openai-completions",
            "models": [
              {
                "id": "minimaxai/minimax-m2.1",
                "name": "NVIDIA MiniMax M2.1",
                "reasoning": true,
                "input": ["text"],
                "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
                "contextWindow": 200000,
                "maxTokens": 131072
              }
            ]
          },
          "ollama-remote": {
            "baseUrl": "http://${OLLAMA_SERVER_IP}:11434/v1",
            "apiKey": "ollama-local",
            "api": "openai-completions",
            "models": [
              {
                "id": "qwen2.5:14b-instruct-q8_0-ctx131072",
                "name": "Qwen2.5 14B Instruct Q8_0 (128K context)",
                "reasoning": false,
                "input": ["text"],
                "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
                "contextWindow": 131072,
                "maxTokens": 131072
              },
              {
                "id": "qwen2.5:32b-instruct-q4_1-ctx64k",
                "name": "Qwen2.5 32B Instruct Q4_1 (64K context)",
                "reasoning": false,
                "input": ["text"],
                "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
                "contextWindow": 65536,
                "maxTokens": 65536
              }
            ]
          }
        }
      },
      "agents": {
        "defaults": {
          "maxConcurrent": 4,
          "subagents": {
            "maxConcurrent": 8
          },
          "compaction": {
            "mode": "safeguard"
          },
          "workspace": "/home/ubuntu/.openclaw/workspace",
          "model": {
            "primary": "nvidia-minimax/minimaxai/minimax-m2.1"
          },
          "models": {
            "nvidia-minimax/minimaxai/minimax-m2.1": {},
            "ollama-remote/qwen2.5:14b-instruct-q8_0-ctx131072": {},
            "ollama-remote/qwen2.5:32b-instruct-q4_1-ctx64k": {}
          }
        }
      }
    }
  },
  "requirements": {
    "openclaw": ">=2026.1.0"
  },
  "install": {
    "steps": [
      {
        "type": "config",
        "description": "設定 Qwen2.5 與 NVIDIA MiniMax 模型配置"
      },
      {
        "type": "instruction",
        "description": "請在 Ollama 伺服器上使用 Modelfile 創建模型，詳見 README.md"
      }
    ]
  },
  "readme": {
    "path": "readme.md",
    "language": "zh_TW"
  }
}
